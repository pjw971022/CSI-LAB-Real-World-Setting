{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPrCVnimE0qR"
      },
      "source": [
        "## Load pre-trained model and processor\n",
        "\n",
        "Let's first apply the image preprocessing and tokenize the text queries using `OwlViTProcessor`. The processor will resize the image(s), scale it between [0-1] range and normalize it across the channels using the mean and standard deviation specified in the original codebase.\n",
        "\n",
        "\n",
        "Text queries are tokenized using a CLIP tokenizer and stacked to output tensors of shape [batch_size * num_max_text_queries, sequence_length]. If you are inputting more than one set of (image, text prompt/s), num_max_text_queries is the maximum number of text queries per image across the batch. Input samples with fewer text queries are padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AD8DXCnJ7faH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-07 10:48:08.673106: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-02-07 10:48:08.707580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-07 10:48:08.707603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-07 10:48:08.708492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-07 10:48:08.714282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-07 10:48:09.458214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "\n",
        "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\", do_pad = False)\n",
        "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
        "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import zmq\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "context = zmq.Context()\n",
        "socket = context.socket(zmq.REQ)  # REQ (REQUEST) 소켓\n",
        "socket.connect(\"tcp://115.145.175.206:5555\")\n",
        "print(\"Client Start\")\n",
        "action = 0 # oracle_action()\n",
        "\n",
        "action_json = json.dumps(action)\n",
        "socket.send_string(action_json)\n",
        "\n",
        "data = socket.recv_string()\n",
        "data = json.loads(data)\n",
        "\n",
        "rgb_array = np.array(data['rgb'])\n",
        "depth_array = np.array(data['depth'])\n",
        "pointcloud_array = np.array(data['pointcloud'])\n",
        "rgb_path = '/home/pjw971022/RealWorldLLM/save_viz/obs/rgb_obs_sample.png'\n",
        "cv2.imwrite(rgb_path, rgb_array)\n",
        "\n",
        "depth_path = '/home/pjw971022/RealWorldLLM/save_viz/obs/depth_obs_sample.png'\n",
        "cv2.imwrite(depth_path, depth_array)\n",
        "print(\"save image!\")\n",
        "\n",
        "# image_path = '/home/pjw971022/RealWorldLLM/save_viz/obs/virtual_test.png'\n",
        "image = Image.open(rgb_path).convert(\"RGB\")\n",
        "# image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "# flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "# flipped_image.save(rgb_path)\n",
        "\n",
        "# Use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from custom_utils.llm_utils import *\n",
        "from PIL import Image\n",
        "\n",
        "llm_agent = LLMAgent(True)\n",
        "obs_img = Image.open('/home/pjw971022/RealWorldLLM/save_viz/obs/rgb_obs_sample.png')\n",
        "# obs_front_img = Image.open('/home/pjw971022/RealWorldLLM/save_viz/obs/packing_shapes_front.jpg')\n",
        "categories_prompt = \\\n",
        "f'Extract only the shapes from the image. ' \\\n",
        "f'Format of objects is <object 1, object 2, object 3>'\n",
        "objects = llm_agent.gemini_generate_categories(categories_prompt, obs_img)\n",
        "joined_objects = \", \".join(objects)\n",
        "obs_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fewshot_prompt = '[Goal] move only square shape in the gray box. [State of Step 1] dice, candy case, pencil, spoon, candy. [Plan 1] move the dice in the gray box. [State of Step 2] candy case, pencil, spoon, candy. [Plan 2] move the candy case in the gray box. [State of Step 3] pencil, spoon, candy. [Plan 3] done cleaning up the table \\n'\n",
        "planning_prompt = f'[Goal] move only spherical shape in the green box. [State of Step 1] {joined_objects} [Plan 1] '\n",
        "task_name = 'real-world-making-word'\n",
        "task_name = task_name.replace('real-world-','').replace('-','_')\n",
        "fewshot_img = Image.open(f'/home/pjw971022/RealWorldLLM/save_viz/obs/{task_name}_fewshot_img.png')\n",
        "plan = llm_agent.gemini_gen_act(fewshot_prompt, planning_prompt, fewshot_img, obs_img)\n",
        "# plan = llm_agent.palm_gen_act(fewshot_prompt, planning_prompt).split('.')[0]\n",
        "plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text queries to search the image for\n",
        "# text_queries = ['baseball', 'cup', 'dice', 'pencil', 'plastic spoon', 'soda can', 'tennis ball']\n",
        "text_queries = objects # ['G', 'O', 'F', 'D', 'E', 'M', 'R']# , \n",
        "# text_queries = [\"circle shape\", \"rectangle shape\",  \"ring shape\", \"square shape\", \"stick shape\"]\n",
        "# text_queries = [\"trash\", \"snack\", \"food\", \"office supplies\", \"toy\", \"tools\", \"kitchenware\", \"fan\"]\n",
        "inputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Print input names and shapes\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val.shape}\")\n",
        "\n",
        "text_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ten5ZJQsoUbE"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Now we can pass the inputs to our OWL-ViT model to get object detection predictions.\n",
        "\n",
        "`OwlViTForObjectDetection` model outputs the prediction logits, boundary boxes and class embeddings, along with the image and text embeddings outputted by the `OwlViTModel`, which is the CLIP backbone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oKBoKQEz_w",
        "outputId": "5deb56c6-c302-4175-a854-d4d6e7b7d903"
      },
      "outputs": [],
      "source": [
        "# Set model in evaluation mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "# Threshold to eliminate low probability predictions\n",
        "score_threshold = 0.1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers.image_utils import ImageFeatureExtractionMixin\n",
        "mixin = ImageFeatureExtractionMixin()\n",
        "\n",
        "# # Load example image\n",
        "image_size = model.config.vision_config.image_size\n",
        "print(image)\n",
        "image = mixin.resize(image, image_size)\n",
        "input_image = np.asarray(image).astype(np.float32) / 255.0\n",
        "\n",
        "target_sizes = torch.Tensor([image.size[::-1]])\n",
        "print(target_sizes)\n",
        "\n",
        "# Get prediction logits\n",
        "logits = torch.max(outputs[\"logits\"][0], dim=-1)\n",
        "scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
        "\n",
        "# Get prediction labels and boundary boxes\n",
        "labels = logits.indices.cpu().detach().numpy()\n",
        "boxes = outputs[\"pred_boxes\"][0].cpu().detach().numpy()\n",
        "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=score_threshold)\n",
        "\n",
        "# print(f\"box: {results[0]['boxes'][2]}\")\n",
        "# print(f\"score: {results[0]['scores'][2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2C5dkQ8sBVZ"
      },
      "source": [
        "## Draw predictions on image\n",
        "\n",
        "Let's draw the predictions / found objects on the input image. Remember the found objects correspond to the input text queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "Xc6YjWXBl_vK",
        "outputId": "23ed2b75-59f0-461b-ce8d-48574de96f92"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(input_image, text_queries, scores, boxes, labels, detected_image_path):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    ax.imshow(input_image, extent=(0, 1.0, 0.75, 0))\n",
        "    ax.set_axis_off()\n",
        "    ax.plot(0., 0.0,'ro')\n",
        "    for score, box, label in zip(scores, boxes, labels):\n",
        "      if score < score_threshold:\n",
        "        continue\n",
        "      else:\n",
        "        print(f'label: {text_queries[label]}     score: {score}    box pose: {box}')\n",
        "      cx, cy, w, h = box\n",
        "      cy *= 0.75\n",
        "      h *= 0.75\n",
        "      ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n",
        "              [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n",
        "      ax.text(\n",
        "          cx - w / 2,\n",
        "          cy + h / 2 + 0.015,\n",
        "          f\"{text_queries[label]}: {score:1.2f}\",\n",
        "          ha=\"left\",\n",
        "          va=\"top\",\n",
        "          color=\"red\",\n",
        "          bbox={\n",
        "              \"facecolor\": \"white\",\n",
        "              \"edgecolor\": \"red\",\n",
        "              \"boxstyle\": \"square,pad=.3\"\n",
        "          })\n",
        "\n",
        "    plt.savefig(detected_image_path)\n",
        "detected_image_path = '/home/pjw971022/RealWorldLLM/save_viz/obs/detected_obs.png'\n",
        "resized_image = cv2.resize(input_image, (640, 480))\n",
        "resized_image.shape\n",
        "\n",
        "plot_predictions(resized_image, text_queries, scores, boxes, labels,detected_image_path)    \n",
        "# plot_predictions(input_image, virtualhome_text_queries, scores, boxes, labels, detected_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_points_within_bbox(point_cloud, bbox):\n",
        "    \"\"\"\n",
        "    Extract points from a point cloud that are within the given bounding box.\n",
        "\n",
        "    Parameters:\n",
        "    - point_cloud (np.array): The point cloud as a numpy array of shape (480, 640, 3).\n",
        "    - bbox (tuple): The bounding box specified as (cx, cy, w, h), where cx and cy are the\n",
        "                    coordinates of the center, and w and h are the width and height.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: A filtered point cloud containing only the points within the bounding box.\n",
        "    \"\"\"\n",
        "    cx, cy, w, h = bbox\n",
        "    cx *= 640\n",
        "    cy *= 480\n",
        "    w *= 640\n",
        "    h *= 480\n",
        "    rows, cols, _ = point_cloud.shape\n",
        "\n",
        "    # Calculate the bounds of the bounding box\n",
        "    x_min = max(int(cx - w / 2), 0)\n",
        "    x_max = min(int(cx + w / 2), cols)\n",
        "    y_min = max(int(cy - h / 2), 0)\n",
        "    y_max = min(int(cy + h / 2), rows)\n",
        "\n",
        "    # Create a mask for points within the bounding box\n",
        "    y_indices, x_indices = np.ogrid[:rows, :cols]\n",
        "    mask = (x_indices >= x_min) & (x_indices < x_max) & (y_indices >= y_min) & (y_indices < y_max)\n",
        "\n",
        "    # Filter the point cloud based on the mask\n",
        "    filtered_points = point_cloud[mask]\n",
        "\n",
        "    return filtered_points\n",
        "\n",
        "g_bbox = [0.88,  0.57,  0.0670583,  0.06833385]\n",
        "y_bbox = [0.52,  0.57,   0.05145311, 0.06556407]\n",
        "r_bbox = [0.52,  0.09, 0.05009488, 0.08601852]\n",
        "\n",
        "def transform_coordinates(x, y): # @\n",
        "    # transform pixel pose with robot coordinate\n",
        "    new_x = y + 0.5\n",
        "    new_y = x - 0.015\n",
        "\n",
        "    return new_x, new_y\n",
        "\n",
        "\n",
        "def transform_franka_pose(bbox, pointcloud_array):\n",
        "    filtered_pc = extract_points_within_bbox(pointcloud_array.reshape(rgb_array.shape), bbox)\n",
        "    filtered_pc = filtered_pc[filtered_pc[:, -1] > 0]\n",
        "    sorted_array_by_z = filtered_pc[filtered_pc[:, -1].argsort()]  # Z 값 기준으로 정렬\n",
        "    bottom_10_by_z = sorted_array_by_z[:1]  # Z 값이 가장 작은 하위 10개 선택\n",
        "    selected_point = np.mean(bottom_10_by_z, axis=0)\n",
        "\n",
        "    # min_idx = filtered_pc[:,-1].argmin()\n",
        "    # selected_point = filtered_pc[min_idx]\n",
        "    # argmin: (0.44239964169263835, 0.1612999939918518) \n",
        "    pose = transform_coordinates(selected_point[0], selected_point[1],)\n",
        "    return pose[0], pose[1], selected_point[2] \n",
        "\n",
        "A_bbox = [0.47674918, 0.6515571, 0.04264363, 0.06411274]\n",
        "A_pose = transform_franka_pose(A_bbox, pointcloud_array)\n",
        "print(A_pose)\n",
        "\n",
        "# g_filtered_pc = extract_points_within_bbox(pointcloud_array.reshape(rgb_array.shape), g_bbox)\n",
        "# y_filtered_pc = extract_points_within_bbox(pointcloud_array.reshape(rgb_array.shape), y_bbox)\n",
        "# r_filtered_pc = extract_points_within_bbox(pointcloud_array.reshape(rgb_array.shape), r_bbox)\n",
        "\n",
        "# g_mask =  g_filtered_pc[:,-1] > 0 \n",
        "# g_min_idx = g_filtered_pc[:,-1][g_mask].argmin() \n",
        "\n",
        "# y_mask =  y_filtered_pc[:,-1] > 0 \n",
        "# y_min_idx = y_filtered_pc[:,-1][y_mask].argmin() \n",
        "\n",
        "# r_mask =  r_filtered_pc[:,-1] > 0 \n",
        "# r_min_idx = r_filtered_pc[:,-1][r_mask].argmin() \n",
        "\n",
        "# g_selected_point = g_filtered_pc[g_min_idx]\n",
        "# y_selected_point = y_filtered_pc[y_min_idx]\n",
        "# r_selected_point = r_filtered_pc[r_min_idx]\n",
        "\n",
        "# print(g_selected_point)\n",
        "# print(y_selected_point)\n",
        "# print(r_selected_point)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# g_pose = transform_coordinates(g_selected_point[0], g_selected_point[1])\n",
        "# y_pose = transform_coordinates(y_selected_point[0], y_selected_point[1])\n",
        "# r_pose = transform_coordinates(r_selected_point[0], r_selected_point[1])\n",
        "\n",
        "# print(g_pose)\n",
        "# print(y_pose)\n",
        "# print(r_pose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "from copy import deepcopy\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def show_point_cloud(points, color_axis, bbox=None,  width_size=1000, height_size=600, coordinate_frame=True):\n",
        "    '''\n",
        "    points : (N, 3) size of ndarray\n",
        "    color_axis : 0, 1, 2\n",
        "    '''\n",
        "    assert points.shape[1] == 3\n",
        "    assert color_axis==0 or color_axis==1 or color_axis==2   \n",
        "    \n",
        "    min_value = 0.0\n",
        "    max_value = 0.1\n",
        "    # color 값을 해당 범위로 제한\n",
        "    clipped_colors = np.clip(points[:, color_axis], min_value, max_value)\n",
        "\n",
        "    # Create a scatter3d Plotly plot\n",
        "    plotly_fig = go.Figure(data=[go.Scatter3d(\n",
        "        x=points[:, 0],\n",
        "        y=points[:, 1],\n",
        "        z=points[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=1,\n",
        "            color=clipped_colors, # Set color based on Z-values\n",
        "            colorscale='jet', # Choose a color scale\n",
        "            colorbar=dict(title='value') # Add a color bar with a title\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    x_range = points[:, 0].max()*0.8 - points[:, 0].min()*0.8\n",
        "    y_range = points[:, 1].max()*0.8 - points[:, 1].min()*0.8\n",
        "    z_range = points[:, 2].max()*0.8 - points[:, 2].min()*0.8\n",
        "\n",
        "    # Adjust the Z-axis scale\n",
        "    plotly_fig.update_layout(\n",
        "        scene=dict(\n",
        "            aspectmode='manual',\n",
        "            aspectratio=dict(x=x_range, y=y_range, z=z_range), # Here you can set the scale of the Z-axis     \n",
        "            camera=dict(\n",
        "                eye=dict(x=0.5, y=0.5, z=0.2)  # 카메라의 위치 조정\n",
        "            )\n",
        "        ),\n",
        "        width=width_size, # Width of the figure in pixels\n",
        "        height=height_size, # Height of the figure in pixels\n",
        "        showlegend=False\n",
        "    )\n",
        "    \n",
        "    if coordinate_frame:\n",
        "        # Length of the axes\n",
        "        axis_length = 1\n",
        "\n",
        "        # Create lines for the axes\n",
        "        lines = [\n",
        "            go.Scatter3d(x=[0, axis_length], y=[0, 0], z=[0, 0], mode='lines', line=dict(color='red')),\n",
        "            go.Scatter3d(x=[0, 0], y=[0, axis_length], z=[0, 0], mode='lines', line=dict(color='green')),\n",
        "            go.Scatter3d(x=[0, 0], y=[0, 0], z=[0, axis_length], mode='lines', line=dict(color='blue'))\n",
        "        ]\n",
        "\n",
        "        # Create cones (arrows) for the axes\n",
        "        cones = [\n",
        "            go.Cone(x=[axis_length], y=[0], z=[0], u=[axis_length], v=[0], w=[0], sizemode='absolute', sizeref=0.1, anchor='tail', showscale=False),\n",
        "            go.Cone(x=[0], y=[axis_length], z=[0], u=[0], v=[axis_length], w=[0], sizemode='absolute', sizeref=0.1, anchor='tail', showscale=False),\n",
        "            go.Cone(x=[0], y=[0], z=[axis_length], u=[0], v=[0], w=[axis_length], sizemode='absolute', sizeref=0.1, anchor='tail', showscale=False)\n",
        "        ]\n",
        "\n",
        "        # Add lines and cones to the figure\n",
        "        for line in lines:\n",
        "            plotly_fig.add_trace(line)\n",
        "        for cone in cones:\n",
        "            plotly_fig.add_trace(cone)\n",
        "\n",
        "\n",
        "    # Add Bounding box in figure\n",
        "    if bbox is not None:\n",
        "        cx,cy,w,h = bbox\n",
        "        bbox_min = [cx-w/2, cy-h/2, 0.0]\n",
        "        bbox_max = [cx+w/2, cy+h/2, 0.1]\n",
        "\n",
        "        bbox_vertices = np.array([\n",
        "            [bbox_min[0], bbox_min[1], bbox_min[2]],\n",
        "            [bbox_min[0], bbox_min[1], bbox_max[2]],\n",
        "            [bbox_min[0], bbox_max[1], bbox_min[2]],\n",
        "            [bbox_min[0], bbox_max[1], bbox_max[2]],\n",
        "            [bbox_max[0], bbox_min[1], bbox_min[2]],\n",
        "            [bbox_max[0], bbox_min[1], bbox_max[2]],\n",
        "            [bbox_max[0], bbox_max[1], bbox_min[2]],\n",
        "            [bbox_max[0], bbox_max[1], bbox_max[2]],\n",
        "        ])\n",
        "\n",
        "        i = [0, 0, 0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 6, 6, 7]\n",
        "        j = [1, 2, 4, 3, 5, 3, 6, 7, 5, 6, 0, 7, 1, 7, 2, 3]\n",
        "        k = [3, 6, 5, 7, 7, 7, 7, 7, 6, 2, 1, 3, 5, 3, 4, 2]\n",
        "        \n",
        "        plotly_fig.add_trace(go.Mesh3d(\n",
        "            x=bbox_vertices[:, 0],\n",
        "            y=bbox_vertices[:, 1],\n",
        "            z=bbox_vertices[:, 2],\n",
        "            i=i,\n",
        "            j=j,\n",
        "            k=k,\n",
        "            color='#FFB6C1',\n",
        "            opacity=0.5\n",
        "        ))\n",
        "\n",
        "\n",
        "    # Show the plot\n",
        "    plotly_fig.show()\n",
        "\n",
        "pc_array = deepcopy(pointcloud_array)\n",
        "pc_array[:,2] = 0.85 - pc_array[:,2] \n",
        "baseball_bbox = [0.68078583, 0.31387693, 0.0932375,  0.1164256 ]\n",
        "tennisball_bbox = [0.7472204,  0.8204705,  0.08430256, 0.11234711]\n",
        "def transform_bbox(bbox):\n",
        "    bbox[0] *= -1\n",
        "    bbox[1] *= -1\n",
        "    bbox[1] *= 0.75\n",
        "    bbox[3] *= 0.75\n",
        "\n",
        "    bbox = [ p * 0.65 for p in bbox]\n",
        "    bbox[0] += 0.6\n",
        "    bbox[1] += 0.1\n",
        "    print(bbox)\n",
        "    return bbox\n",
        "\n",
        "# transform_bbox(baseball_bbox)\n",
        "# transform_bbox(tennisball_bbox)\n",
        "\n",
        "# new_x, new_y = transform_coordinates(baseball_bbox[0], baseball_bbox[1])\n",
        "# baseball_bbox[0] = new_x\n",
        "# baseball_bbox[1] = new_y\n",
        "# 0.17, -0.12\n",
        "# 0 , -0.3\n",
        "show_point_cloud(pc_array, 2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "OWL-ViT-inference example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
