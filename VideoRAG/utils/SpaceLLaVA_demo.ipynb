{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ2NIT7Tj5ni",
        "outputId": "54b6f79a-9821-427f-b97f-2748b30e8f4c"
      },
      "outputs": [],
      "source": [
        "# !export CUDA_HOME=/usr/local/cuda \n",
        "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \n",
        "# !export PATH=$PATH:$CUDA_HOME/bin \n",
        "# !CUDACXX=/usr/local/cuda-11.6/bin/nvcc CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=native\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8RZjwdLpo3Y"
      },
      "source": [
        "## Downloading SpaceLLaVA GGUF weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pycuda.driver as cuda\n",
        "# cuda.init()\n",
        "\n",
        "# for i in range(cuda.Device.count()):\n",
        "#     gpu = cuda.Device(i)\n",
        "#     compute_capability = f\"{gpu.compute_capability_major}.{gpu.compute_capability_minor}\"\n",
        "#     print(f\"GPU {i}: {gpu.name()}, Compute Capability: {compute_capability}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d8h5ZcWQj-_m"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from llama_cpp import Llama\n",
        "from llama_cpp.llama_chat_format import Llava15ChatHandler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb9pLwRyrqpx"
      },
      "source": [
        "### Helper funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RlRqhWSckCAn"
      },
      "outputs": [],
      "source": [
        "def image_to_base64_data_uri(image_input):\n",
        "    # Check if the input is a file path (string)\n",
        "    if isinstance(image_input, str):\n",
        "        with open(image_input, \"rb\") as img_file:\n",
        "            base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
        "\n",
        "    # Check if the input is a PIL Image\n",
        "    elif isinstance(image_input, Image.Image):\n",
        "        buffer = io.BytesIO()\n",
        "        image_input.save(buffer, format=\"PNG\")  # You can change the format if needed\n",
        "        base64_data = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Input must be a file path or a PIL.Image.Image instance.\")\n",
        "\n",
        "    return f\"data:image/png;base64,{base64_data}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget -O ggml-model-q4_0.gguf https://huggingface.co/remyxai/SpaceLLaVA/resolve/main/ggml-model-q4_0.gguf?download=true\n",
        "# !wget -O mmproj-model-f16.gguf https://huggingface.co/remyxai/SpaceLLaVA/resolve/main/mmproj-model-f16.gguf?download=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hNbhokRpwoN"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QrMSuNN6kYKE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clip_model_load: loaded meta data with 19 key-value pairs and 377 tensors from /home/pjw971022/.cache/spatial_vlm/mmproj-model-f16.gguf\n",
            "clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "clip_model_load: - kv   0:                       general.architecture str              = clip\n",
            "clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false\n",
            "clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true\n",
            "clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true\n",
            "clip_model_load: - kv   4:                          general.file_type u32              = 1\n",
            "clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336\n",
            "clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA\n",
            "clip_model_load: - kv   7:                        clip.projector_type str              = mlp\n",
            "clip_model_load: - kv   8:                     clip.vision.image_size u32              = 336\n",
            "clip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14\n",
            "clip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1024\n",
            "clip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4096\n",
            "clip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 768\n",
            "clip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16\n",
            "clip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\n",
            "clip_model_load: - kv  15:                    clip.vision.block_count u32              = 23\n",
            "clip_model_load: - kv  16:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\n",
            "clip_model_load: - kv  17:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\n",
            "clip_model_load: - kv  18:                              clip.use_gelu bool             = false\n",
            "clip_model_load: - type  f32:  235 tensors\n",
            "clip_model_load: - type  f16:  142 tensors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 2 CUDA devices:\n",
            "  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n",
            "  Device 1: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "mmproj=\"/home/pjw971022/.cache/spatial_vlm/mmproj-model-f16.gguf\"\n",
        "model_path=\"/home/pjw971022/.cache/spatial_vlm/ggml-model-q4_0.gguf\"\n",
        "chat_handler = Llava15ChatHandler(clip_model_path=mmproj, verbose=False)\n",
        "spacellava = Llama(model_path=model_path, chat_handler=chat_handler, n_ctx=2048, logits_all=True, n_gpu_layers = -1, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tlrFwnCMk1aO"
      },
      "outputs": [],
      "source": [
        "# @title Set your image and prompt\n",
        "image_path = '/home/pjw971022/workspace/Sembot/VideoRAG/Database/custom_sample/warehouse/warehouse_test.jpg' # @param {type:\"string\"}\n",
        "prompt = \"how high is the stack of boxes on the pallet held up by the forklift?\" # @param {type:\"string\"\n",
        "# prompt = \"Tell me what the object at the front among the items on the desk is. There is several objects on it, including a bookshelf, a spam can, a mustard, domino sugar, and a crackers.\"\n",
        "# prompt = \"Please estimate the distance from the robotic arm to each object on the table. The distance to the table is 90cm. There is several objects on it, including a bookshelf, a spam can, a mustard, domino sugar, and a crackers.\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjUMSt4nkeye"
      },
      "outputs": [],
      "source": [
        "if '.' in image_path.split('/')[-1]:\n",
        "    # Load the image and convert it to base64 data uri\n",
        "    image_path = image_path \n",
        "    data_uri = image_to_base64_data_uri(image_path)\n",
        "    system_query = \"\"\"You are participating in a visual question answering game with your\n",
        "    friend. In this game, you are presented with a question which requires visual information from an\n",
        "    image to answer. You can see the question but not the image, while your friend 1 can see the image but\n",
        "    not the original question. Luckily, you are allowed to decompose the question and ask your friend\n",
        "    about the image. Your friend gives you answers which can be used to answer the original question.\n",
        "    \"\"\"\n",
        "    CoT_example = \"\"\"\n",
        "    Here is a sample conversation:\n",
        "    [Question] How can I clean up the table? Give detailed instruction about how should I move my hand.\n",
        "    [You] What objects are there in the image?\n",
        "    [Friend] There is an empty coke can, a trash bin and a coffee machine.\n",
        "    [You] Is the trash bin to the left or to the right of the coke can?\n",
        "    [Friend] It's to the left.\n",
        "    [You] Is the trash bin or the coke can further from you?\n",
        "    [Friend] They are similar in depth.\n",
        "    [You] How much to the left is the trash bin compared to the coke can?\n",
        "    [Friend] Around 20 centimeters.\n",
        "    [Answer] One should grab the coke can, move it 20 centimeters left and release it so it falls in the trash bin.\n",
        "    Here is another example:\n",
        "    [Question] Tell me if the distance\n",
        "    between the blue bottle and the yellow book is longer than that between the plant and the coke can?\n",
        "    [You] What is the distance between the blue bottle and the yellow book?\n",
        "    [Tool] 0.3m\n",
        "    [You] What is the distance between the plant and the coke can?\n",
        "    [Friend] 0.7m\n",
        "    [Robot] Since the distance between the blue bottle and the\n",
        "    yellow book is 0.3m and distance between the plant while the coke can is 0.7m, the distance between\n",
        "    the blue bottle and the yellow book is not longer than that between the plant and the coke can.\n",
        "    [Answer] No.\n",
        "    Here is another example:\n",
        "    [Question] Which object can be reached by kids more easily, the white and yellow rabbit toy can or the dark green can of beer?\n",
        "    [You] What is the elevation of the white and yellow rabbit toy can?\n",
        "    [Friend] 0.9 m.\n",
        "    [You] What is the elevation of the dark green can of beer?\n",
        "    [Friend] 0.2 m.\n",
        "    [Answer] Since the kids are generally shorter, it is easier for them to reach something that are lower in altitude, so it would be easier for them to reach the can of beer.\n",
        "    Now, given a new question, try to answer the questions by asking your friend for related visual information.\n",
        "    [Question]\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_query},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
        "                {\"type\" : \"text\", \"text\": prompt}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    results = spacellava.create_chat_completion(messages = messages, temperature=0.2)\n",
        "    file_name = image_path.split('/')[-1]\n",
        "    print(f\"file: {file_name}\",results[\"choices\"][0][\"message\"][\"content\"])\n",
        "# else:\n",
        "#     import glob\n",
        "#     for image_file_path in glob.glob(image_path + '/*'):\n",
        "#         data_uri = image_to_base64_data_uri(image_file_path)\n",
        "#         messages = [\n",
        "#             {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
        "#             {\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"content\": [\n",
        "#                     {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
        "#                     {\"type\" : \"text\", \"text\": prompt}\n",
        "#                     ]\n",
        "#                 }\n",
        "#             ]\n",
        "        \n",
        "#         results = spacellava.create_chat_completion(messages = messages, temperature=0.2)\n",
        "#         file_name = image_file_path.split('/')[-1]\n",
        "#         print(f\"file: {file_name}\",results[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
