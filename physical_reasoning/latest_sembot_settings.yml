video_vlm_model:
  metadata_path: /home/mnt/data/video_rag_database/embedding_data/CharadesEgo_emb_only_metadata.csv
  video_folder_path: /home/mnt/data/video_rag_database/CharadesEgo/CharadesEgo_v1_480

video_vlm_prompt:
  system: |
    you are a helpful language and video assistant. you have to focus on the actions of the arms and hands of the person in the video.
  prompt_demo_extractor: |
    [primitive actions] move, grasp, release with the hand, rotate 
    Please explain this video in detail. When explaining, you must pair [primitive actions] with the context of the action. Example: <move to the cabinet> - <To open a cabinet with a handle, move your hand towards the cabinet."""
    
llm_prompt:
  system: |
    You are a participant in a game where you must create fine-grained commands for a robotic arm placed in a tabletop environment. Since you cannot see the environment or reference human actions on your own, you will be provided with two friends to assist you.
  fewshot: |
    In this game, you must generate "fine-grained instruction" as [Answer] based on human video and current environment. All questions you ask your friends must pertain to the object involved in the command or key features of the related action. you can only observe the environment through your friend 1. On the other hand, since Friend 2 can watch videos of human performing actions, you must ask Friend 2 in order to learn the correct behaviors to execute the commands.

    Now, given a new question, try to answer the questions by asking your friends for related visual information and requesting modifications the environment.


llava_prompt_3d:
  system: |
    You are a helpful language and vision assistant. Each time, the user will provide a first-person, egocentric image and a list of objects that are highly likely to appear in the image. Not all objects might show up in the image because they could be occluded. If you don't see an object from the object list, you should be honest and say that you haven't seen that object. You must not describe any objects that are not listed. In your responses, you must answer the question that the human has specific to this scene and the objects listed.
  question_template: |
    <image>\nYou must only mention these objects in your answer: <obj_list>. <question>

llava_prompt_2d:
  system: |
    You are a helpful language and vision assistant. Each time, the user will provide a first-person, egocentric image and a list of objects that are highly likely to appear in the image. Not all objects might show up in the image because they could be occluded. If you don't see an object from the object list, you should be honest and say that you haven't seen that object. You must not describe any objects that are not listed. In your responses, you must answer the question that the human has specific to this scene and the objects listed.
  question_template: |
    <image>\nYou must only mention these objects in your answer: <obj_list>. <question>


physical_mode: 3d
api_mode: google
openai_settings:
  model: gpt-4-1106-preview
  temperature: 1
  max_tokens: 2049
google_settings:
  model: gemini-1.0-pro-latest
  temperature: 1
  max_tokens: 2049
llava_model_2d:
  mmproj: "/home/pjw971022/.cache/spatial_vlm/mmproj-model-f16.gguf"
  model_path: "/home/pjw971022/.cache/spatial_vlm/ggml-model-q4_0.gguf"
  temperature: 0.2
llava_model_3d:
  model_path: qizekun/ShapeLLM_13B_general_v1.0
  model_base: 
  device: 
  temperature: 0.2
  max_new_tokens: 512
  load_8bit: False
  load_4bit: False
  conv_mode:
  # model_path: liuhaotian/llava-v1.5-13b-lora # liuhaotian/llava-v1.5-13b-lora
  # model_base: remyxai/SpaceLLaVA # liuhaotian/llava-v1.6-34b liuhaotian/llava-v1.6-vicuna-7b remyxai/SpaceLLaVA
  # conv_mode: spatial_vlm
  