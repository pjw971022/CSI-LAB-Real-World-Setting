{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/pjw971022/Sembot/ravens/')\n",
    "sys.path.append('/home/pjw971022/Sembot')\n",
    "\n",
    "from ravens import tasks\n",
    "from ravens.environments.environment_real import RealEnvironment\n",
    "from ravens.utils import utils\n",
    "\n",
    "from real_bot.perception.detection_agent import ObjectDetectorAgent\n",
    "from real_bot.perception.get_speech import speech_to_command\n",
    "\n",
    "from custom_utils.llm_utils import *\n",
    "import numpy as np\n",
    "import prompts\n",
    "import os\n",
    "from prompts.realworld.object_setup import OBJECT_DICT\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import hydra\n",
    "from PIL import Image\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def main():\n",
    "    cfg = OmegaConf.load('/home/pjw971022/Sembot/real_bot/rw_config/inference.yaml')  # 구성 파일 경로 지정\n",
    "    return cfg\n",
    "cfg = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['plan_mode'] = 'pre-infered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = RealEnvironment(task_name=cfg['task'])\n",
    "task = tasks.names[cfg['task']]()\n",
    "llm_agent = LLMAgent()\n",
    "print(f'@@@ LLM type: {cfg.llm_type} plan mode: {cfg.plan_mode}')\n",
    "\n",
    "task_name = cfg['task'].replace('real-world-','').replace('-','_')\n",
    "agent = ObjectDetectorAgent(task=task_name)\n",
    "agent.detector.model.eval()\n",
    "\n",
    "step_cnt = 1\n",
    "np.random.seed(12)\n",
    "env.seed(12)\n",
    "env.set_task(task)\n",
    "# info = env.info\n",
    "done = False\n",
    "plan_list = None\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_goal = \"clean up the table\" # speech_to_command()  # \"grab the object I'm showing you.\"\n",
    "demo_context = None\n",
    "obs_context = None\n",
    "command_modality = 0\n",
    "show_video_time = 20\n",
    "task_setting_time = 10\n",
    "# 0: speech, 1: vision + speech, 2: video + speech, 3: video + image + speech\n",
    "# if 'show' in final_goal: # Pick up the object I'm showing you.\n",
    "#     print(\"show the image ... 10 seconds wait.\")\n",
    "#     env.send_display_server(1, 'start')\n",
    "#     # env.get_speech()\n",
    "#     command_modality = 1\n",
    "#     env.get_obs_human_cam(1)\n",
    "#     env.send_display_server(1, 'end')\n",
    "    \n",
    "# elif 'imitate' in final_goal:\n",
    "#     print(f\"show the video ... {show_video_time} seconds wait.\")\n",
    "#     env.send_display_server(1, 'start')\n",
    "#     # env.get_speech()\n",
    "#     command_modality = 2\n",
    "#     env.get_obs_human_cam(2)\n",
    "#     env.send_display_server(1, 'end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cls = prompts.names[cfg['task']]()\n",
    "fewshot_prompt = prompt_cls.prompt(command_modality)\n",
    "# env.send_display_server(2, 'start')\n",
    "time.sleep(task_setting_time)\n",
    "# env.send_display_server(2, 'end')\n",
    "\n",
    "goal_name = final_goal.split(' ')[0]\n",
    "objects = [\"coffee stick\", \"first drawer handle\", \"second drawer handle\", \"red cup\", \"blue cup\", \"green basket\", \"setting point1\", \"setting point2\"]\n",
    "objects_str = ', '.join(objects)\n",
    "\n",
    "# env.send_display_server(3, 'start')\n",
    "obs, info_dict = env.reset()\n",
    "\n",
    "obs_img = Image.open(f'/home/pjw971022/Sembot/real_bot/save_vision/obs/image_obs_{step_cnt}.png')\n",
    "if not cfg['plan_mode'] =='pre-infered':\n",
    "    if command_modality == 1:\n",
    "        prompt_extract_state = env.task.prompt_extract_state + f'All possible objects: {objects_str}'\n",
    "        obs_context = llm_agent.gemini_generate_context(prompt_extract_state, obs_img)\n",
    "\n",
    "        human_img = Image.open('/home/pjw971022/Sembot/real_bot/save_vision/obs/human_image.png')\n",
    "        prompt_extract_goal = f\"[Final goal] {final_goal}. Please change [Final goal] to a command that includes a specific object. Example) pick up the red dice.\"  + f'All possible objects: {objects_str}'\n",
    "        final_goal = llm_agent.gemini_generate_context(prompt_extract_goal, human_img)\n",
    "\n",
    "    elif command_modality == 2:\n",
    "        prompt_extract_demo = \"Please explain the human behavior in this video.\"\n",
    "        prompt_extract_demo += f'\\nAll possible objects: {objects_str}'\n",
    "        demo_context = llm_agent.gemini_generate_video_context(prompt_extract_demo, 'demo_glasses.mp4')\n",
    "\n",
    "    elif command_modality == 3:\n",
    "        prompt_extract_state = env.task.prompt_extract_state + f'All possible objects: {objects_str}'\n",
    "        obs_context = llm_agent.gemini_generate_context(prompt_extract_state, obs_img)\n",
    "        \n",
    "        prompt_extract_demo = f\"Please describe the actions of the person appearing in the video. {fewshot_prompt}\"  + f'All possible objects: {objects_str}' + 'Possible Actions: move, pick, place, rotate, push, pull, sweep.'\n",
    "        demo_context = llm_agent.gemini_generate_video_context(prompt_extract_demo, 'demo_glasses.mp4')\n",
    "\n",
    "        human_img = Image.open('/home/pjw971022/Sembot/real_bot/save_vision/obs/human_image.png')\n",
    "        prompt_extract_goal = f\"[Final goal] {final_goal}. Please change [Final goal] to a command that includes a specific object. Example) pick up the red dice.\"  + f'All possible objects: {objects_str}'\n",
    "        final_goal = llm_agent.gemini_generate_context(prompt_extract_goal, human_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    obs_img = Image.open(f'/home/pjw971022/Sembot/real_bot/save_vision/obs/image_obs_{step_cnt}.png')\n",
    "    if cfg.plan_mode == 'closed_loop': # from LLM \n",
    "        prompt_extract_state = env.task.prompt_extract_state + f'All possible objects: {objects_str}'\n",
    "        obs_context = llm_agent.gemini_generate_context(prompt_extract_state, obs_img)\n",
    "    text_context = '[Context] '\n",
    "    ##############################################################################\n",
    "    if (cfg.plan_mode == 'open_loop') and step_cnt == 1:\n",
    "        if demo_context is not None:\n",
    "            print(\"demo_context: \", demo_context)\n",
    "            text_context += f'{demo_context}\\n'\n",
    "        if obs_context is not None:\n",
    "            print(f\"@@@ Context from vision: {obs_context}\")\n",
    "            text_context += f'{obs_context}\\n' \n",
    "    text_context += f'All possible objects: {objects_str}'\n",
    "    text_context += 'Possible Actions: move, pick, place, rotate, push, pull, sweep.'\n",
    "    planning_prompt = \\\n",
    "    f'[Goal] {final_goal} {text_context}'\n",
    "    \n",
    "    if cfg.plan_mode == 'closed_loop':\n",
    "        joined_categories = \", \".join(objects)\n",
    "        planning_prompt = f'{planning_prompt}' \\\n",
    "                        f'[State of Step {step_cnt}] {joined_categories} '                  \n",
    "    else:\n",
    "        planning_prompt = f'{planning_prompt}'\n",
    "\n",
    "    ##############################################################################\n",
    "    if cfg.plan_mode == 'closed_loop':\n",
    "        planning_prompt +=  f'[Plan {step_cnt}] '\n",
    "        if cfg.llm_type == 'gemini':\n",
    "            gen_act = llm_agent.gemini_gen_act(fewshot_prompt, planning_prompt, obs_img)\n",
    "        elif cfg.llm_type == 'gpt4':\n",
    "            gen_act = llm_agent.gpt4_gen_act(fewshot_prompt, planning_prompt)\n",
    "        lang_action = gen_act\n",
    "\n",
    "    elif cfg.plan_mode == 'open_loop' and step_cnt == 1:\n",
    "        if plan_list == None:\n",
    "            if cfg.llm_type == 'gemini':\n",
    "                plan_list = llm_agent.gemini_gen_all_plan(fewshot_prompt, planning_prompt)\n",
    "            elif cfg.llm_type == 'gpt4':\n",
    "                plan_list = llm_agent.gpt4_gen_all_plan(fewshot_prompt, planning_prompt)\n",
    "        lang_action = plan_list[step_cnt-1]\n",
    "    else:\n",
    "        plan_list = [\"move the <mouse> to the <mouse pad>\",\n",
    "                     \"move the <coke> to the <trash can>\",\n",
    "                     \"move the <trash> to the <trash can>\",\n",
    "                     \"move the <book> to the <bookshelf>\",\n",
    "                     \"move the <pencil> to the <pencil holder>\",\n",
    "                     ]\n",
    "\n",
    "        lang_action = plan_list[step_cnt-1]\n",
    "    print(f\"Plan: {lang_action}\")\n",
    "    if ('done' in lang_action) or ('Done' in lang_action):\n",
    "        print(\"Task End!!!\")\n",
    "        # env.send_display_server(4, 'end')\n",
    "        env.server_close()\n",
    "        break\n",
    "\n",
    "    elif env.task.max_steps < step_cnt:\n",
    "        break\n",
    "\n",
    "    obs['objects'] = objects\n",
    "    obs['lang_action'] = lang_action\n",
    "    obs['step_cnt'] = step_cnt\n",
    "    act = agent.forward(obs)\n",
    "    step_cnt += 1\n",
    "    # env.send_display_server(3, 'end')\n",
    "\n",
    "    if act == -1:\n",
    "        continue\n",
    "    # if step_cnt == 1:\n",
    "    #     env.send_display_server(4, 'start')\n",
    "    # Continue with the rest of the code\n",
    "    \n",
    "    z = env.step(act)\n",
    "    try:\n",
    "        obs, reward, done, info = z\n",
    "    except ValueError:\n",
    "        obs, reward, done, info, action_ret_str = z\n",
    "        print(action_ret_str)\n",
    "\n",
    "    planning_prompt = f'{planning_prompt}{lang_action}. '\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
